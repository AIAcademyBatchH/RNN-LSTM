{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJ_2_lVptmQb"
   },
   "source": [
    "In the code demo, we will talk about how we can use embedding layers in keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7fc0gKPasaZw",
    "outputId": "447e0214-a0c5-46f8-ec28-0d54139375e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "Em9QsxJDsZrR"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "BASE_DIR=\"/content/gdrive/MyDrive/RNN-LSTM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "p8StQn7wsZrX"
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv(os.path.join(BASE_DIR,'headlines.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "ODfaef5wsZrX",
    "outputId": "b2b5c798-8aa9-40aa-b83a-27308e751909"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>CATEGORY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>226435</td>\n",
       "      <td>Google+ rolls out 'Stories' for tricked out ph...</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>356684</td>\n",
       "      <td>Dov Charney's Redeeming Quality</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>246926</td>\n",
       "      <td>White God adds Un Certain Regard to the Palm Dog</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>318360</td>\n",
       "      <td>Google shows off Androids for wearables, cars,...</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>277235</td>\n",
       "      <td>China May new bank loans at 870.8 bln yuan</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID                                              TITLE CATEGORY\n",
       "0  226435  Google+ rolls out 'Stories' for tricked out ph...        t\n",
       "1  356684                    Dov Charney's Redeeming Quality        b\n",
       "2  246926   White God adds Un Certain Regard to the Palm Dog        e\n",
       "3  318360  Google shows off Androids for wearables, cars,...        t\n",
       "4  277235         China May new bank loans at 870.8 bln yuan        b"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4m-Jel8ttxj3"
   },
   "source": [
    "This data contains the news headlines as well as your corresponding categories. There are in total four unique categories. These categories are about news belonging to technology, business etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1e6qgHYPsZrZ"
   },
   "outputs": [],
   "source": [
    "## We will create a classifier using embedding layer architecture\n",
    "X=train['TITLE']\n",
    "y=train['CATEGORY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "z2rUJn38sZra"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7PK4irCBsZra"
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DeyVKxAesZrb"
   },
   "outputs": [],
   "source": [
    "enc=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "uucjM0ZJsZrc"
   },
   "outputs": [],
   "source": [
    "y_train=enc.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RzbDJ9WqsZrd",
    "outputId": "8ec0d811-d9b7-4cbc-b82a-12d6be41851d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['b', 'e', 'm', 't'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## the unique labels in the category columns\n",
    "enc.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VlPes6n2sZrd",
    "outputId": "255a1cc2-84bc-4baf-e88e-53b0ede120e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 3, ..., 3, 1, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMSGb52mua3Y"
   },
   "source": [
    "Now, we will turn our attention to the data stored in X_train and X_test parts. If you remember this is the data about News headlines which we will be using as a predictors. Now as mentioned earlier, we are using an embedding layer. So we will need to do some data preparation in order to use our embedding layer which is we will need to convert a text into a sequence of numbers and then we will have to truncate those numbers to a particular length. For that, we will be using the Tokenizer class We will be using the pad sequences() function to zero pad our sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "OExaT4cUsZre"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "iZGbGHlmsZrf"
   },
   "outputs": [],
   "source": [
    "seq_len=16\n",
    "max_words=10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlwSWnQCuudk"
   },
   "source": [
    "So I have given a number 16 to a variable called sequence length. This is going to be a setting for deciding how long each sequence would be. So I've truncated my sequences to a length of at max 16. Any sequence which is less than 16 will be zero padded, any sequences which is more than 16 words will be truncated. This is a setting for the vocabulary. Now here I'm saying that I only want to consider 10,000 unique words in my corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "SL2xSlc3sZrf"
   },
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(num_words=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "SRHGw0-QsZrg"
   },
   "outputs": [],
   "source": [
    "### Split the text into words and assign an integer id\n",
    "tokenizer.fit_on_texts(X_train.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjXj9F87vJAJ"
   },
   "source": [
    "And creating an object of tokenizer class and then I'm fitting my train data on it. What this will do is: This will create an integer index for each word in my corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6_H0VdRLsZrg",
    "outputId": "ae98a6b1-8f38-4be6-87ca-659fc4e5d828"
   },
   "outputs": [],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aznQZ8iZvTB_"
   },
   "source": [
    "Now, I will create integer sequences on my train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "roDtFYFMsZrg"
   },
   "outputs": [],
   "source": [
    "## Create a sequence for each entry in the title column\n",
    "sequence=tokenizer.texts_to_sequences(X_train.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mQ9TnKaqsZrh",
    "outputId": "8f01cd4c-921f-412d-add9-2cc6e88a1b5e"
   },
   "outputs": [],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdKRRdNyvoFQ"
   },
   "source": [
    "Now each of the sentence in my train data has now been converted into a sequence of integers according to this mapping. Now, you can see that not all the sequences are of same length. We need to make them of same length the way we do that is by using pad sequences () method and giving it a value off sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "0vEu51rxsZrh"
   },
   "outputs": [],
   "source": [
    "## Pad the sequences\n",
    "train_features=pad_sequences(sequence,maxlen=seq_len)\n",
    "\n",
    "#default is pre\n",
    "## parameters: padding and truncating - can give post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UVmUScRasZri",
    "outputId": "40af5e09-6078-44b8-e9b1-779dc5497c95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,  142, 1562, 8052],\n",
       "       [   0,    0,    0, ...,    4, 1671,  525],\n",
       "       [   0,    0,    0, ..., 5370,    6,   47],\n",
       "       ...,\n",
       "       [   0,    0,    0, ..., 4732, 1042,  359],\n",
       "       [   0,    0,    0, ...,   46,   41,   80],\n",
       "       [   0,    0,    0, ..., 2953, 6426, 2189]], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "URjWykWTsZri",
    "outputId": "cf6a76e1-9a88-4edf-9d75-8b7d5cc16d63"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168967, 16)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJv3vNO-v1-d"
   },
   "source": [
    "Do the same for test also:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "9pRClifnsZri"
   },
   "outputs": [],
   "source": [
    "## Create test features\n",
    "sequence=tokenizer.texts_to_sequences(X_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cd59x5WasZrj",
    "outputId": "1792fffd-0e93-4f23-d1c5-7318d7eda164"
   },
   "outputs": [],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "YFwzBlvnsZrj"
   },
   "outputs": [],
   "source": [
    "test_features=pad_sequences(sequence,maxlen=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H6xi5PuHsZrk",
    "outputId": "9fcd8729-df8a-4ccf-e939-7ddcd832a4a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,  113,    2,   31],\n",
       "       [   0,    0,    0, ...,    4, 4018, 3115],\n",
       "       [   0,    0,    0, ...,  375, 5948, 4400],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,   11,  157, 1648],\n",
       "       [   0,    0,    0, ...,   97,   76,    7],\n",
       "       [   0,    0,    0, ...,  310, 3979, 5986]], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kmm44OJXsZrk",
    "outputId": "88b1b43f-32c9-424d-abfb-f2055bc76629"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42242, 16)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvnpmqoGwUEj"
   },
   "source": [
    "Now at this point if you remember the Y vector that we had contains the integer labels. Now in order to use a target variables within keras you need to one hot encoded if it is not binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "L1ENdMQ9sZrk"
   },
   "outputs": [],
   "source": [
    "## Convert y_test and y_train to one hot encoded vector\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "1j0OTEG8sZrk"
   },
   "outputs": [],
   "source": [
    "y_train=to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x0Abxii0sZrl",
    "outputId": "b75076e7-9fb2-41fd-e9fc-373cfb351a6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "R6-DTV9asZrl"
   },
   "outputs": [],
   "source": [
    "## Assemble the model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHyA3RlEwbt5"
   },
   "source": [
    "So I'm importing the sequential class and I will be using dense layers. I will be using embedding layers and in order to connect Dense and embedding layers I will have to flatten them, so I’m also importing the flatten layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "MR9P4506sZrl"
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(input_dim=max_words,output_dim=64,input_length=seq_len))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024,activation='relu'))\n",
    "model.add(Dense(4,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IY5kgSbwvSk"
   },
   "source": [
    "Now input_dim defines the number of rows in the embedding layer. Now if you remember the maximum words that we chosen was 10,000. So our embedding layer will have 10,000 rows. Here, we are saying that this embedding layer will output vectors with a dimension of 64. So each vector will be of length 64 and here we are specifying the length of the inputs that will go to an embedding layer. Now if you remember the inputs that will go to embedding layer are my train features and each of them as a length of 16. Next, I will flatten this layer to connect it to a dense layer which has 1024 cells and each cell has a relu activation. The last layer contains four cells because my target variable only has four unique values and its activation is softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0JBEvI67sZrl",
    "outputId": "6124f038-2d93-4953-aec6-fe47fd7e82d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 16, 64)            640000    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 4100      \n",
      "=================================================================\n",
      "Total params: 1,693,700\n",
      "Trainable params: 1,693,700\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "01oTJkxBsZrm"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-4pLJ4YXsZrm",
    "outputId": "34cb123d-eb73-4778-f543-ff0f908ebe84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4225/4225 [==============================] - 70s 16ms/step - loss: 0.3481 - accuracy: 0.8786 - val_loss: 0.2707 - val_accuracy: 0.9106\n",
      "Epoch 2/3\n",
      "4225/4225 [==============================] - 77s 18ms/step - loss: 0.2562 - accuracy: 0.9203 - val_loss: 0.2899 - val_accuracy: 0.9095\n",
      "Epoch 3/3\n",
      "4225/4225 [==============================] - 69s 16ms/step - loss: 0.2485 - accuracy: 0.9246 - val_loss: 0.3177 - val_accuracy: 0.9086\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3321369110>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_features,y_train,epochs=3,batch_size=32,validation_split=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-65oxyWJxe3u"
   },
   "source": [
    "This will take some time to run. You can see now that the model has been trained we're getting a 91% accuracy on our validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "qUq1LSy1sZrm"
   },
   "outputs": [],
   "source": [
    "preds=model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "djTlnFzisZrm",
    "outputId": "2c3b98f3-65ad-4431-af9c-3adf00054d2f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.6802810e-01, 2.5659227e-05, 1.5594635e-05, 1.3193056e-01],\n",
       "       [3.9068091e-01, 3.7802190e-05, 3.2304013e-06, 6.0927808e-01],\n",
       "       [1.5176908e-05, 4.4397647e-10, 1.3902290e-10, 9.9998486e-01],\n",
       "       ...,\n",
       "       [1.0000000e+00, 4.4385003e-11, 2.0820911e-13, 2.3511115e-09],\n",
       "       [4.5210958e-01, 2.5955535e-02, 9.2327716e-03, 5.1270217e-01],\n",
       "       [2.9743867e-06, 1.5632887e-07, 5.0865716e-09, 9.9999690e-01]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9wDKG3-yWgw"
   },
   "source": [
    "This is how our predictions look like. Now these predictions are in the form of integers. We will need to decode them for that we will use the inverse_transform() method in our encoded class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "AImnFQZQ0Agj"
   },
   "outputs": [],
   "source": [
    "max_labels = []\n",
    "for i in preds:\n",
    "  max_labels.append(np.argmax(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fbGDCBj50XC8",
    "outputId": "47cd54c9-8bdf-40a6-b16f-43b70537f482"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 3, ..., 0, 3, 3])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(max_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "twz-UzETsZrm"
   },
   "outputs": [],
   "source": [
    "pred_labels=enc.inverse_transform(np.array(max_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pco76tmVsZrn",
    "outputId": "0e11afb4-fb4c-4e98-fb19-cf48fec54cc3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['b', 't', 't', ..., 'b', 't', 't'], dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnMyVp-EzfX9"
   },
   "source": [
    "Now we can see the predicted labels. Let’s see the accuracy on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t7_lTIZJsZrn",
    "outputId": "5b249e53-71b0-4bbf-8255-3b53ac2924e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9102552])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_test==pred_labels).sum()/pred_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxqAfUjWziXc"
   },
   "source": [
    "the accuracy on the test data is around 91%.\n",
    "So in this way, we can include an embedding layer to text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sOAGR7ggsZrn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Embedding_layer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
