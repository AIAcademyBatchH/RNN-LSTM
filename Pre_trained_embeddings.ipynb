{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SC2C98eA7ej1"
   },
   "source": [
    "In this code demo, we will see how we can use Pre-trained word vectors to populate the embedding matrix and then use it to train a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3OvDXiSy0_CP",
    "outputId": "94687f7c-2fb1-4dd8-a5f4-92cd64758f57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JHiIm7MH086w"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "BASE_DIR=\"/content/gdrive/MyDrive/RNN-LSTM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2y0VVjGU0861"
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv(os.path.join(BASE_DIR,'headlines.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "wLly4hmX0862",
    "outputId": "c02a66eb-24e8-42dd-db00-58b8b098beb5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>CATEGORY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>226435</td>\n",
       "      <td>Google+ rolls out 'Stories' for tricked out ph...</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>356684</td>\n",
       "      <td>Dov Charney's Redeeming Quality</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>246926</td>\n",
       "      <td>White God adds Un Certain Regard to the Palm Dog</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>318360</td>\n",
       "      <td>Google shows off Androids for wearables, cars,...</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>277235</td>\n",
       "      <td>China May new bank loans at 870.8 bln yuan</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID                                              TITLE CATEGORY\n",
       "0  226435  Google+ rolls out 'Stories' for tricked out ph...        t\n",
       "1  356684                    Dov Charney's Redeeming Quality        b\n",
       "2  246926   White God adds Un Certain Regard to the Palm Dog        e\n",
       "3  318360  Google shows off Androids for wearables, cars,...        t\n",
       "4  277235         China May new bank loans at 870.8 bln yuan        b"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fWuVjEbF0864"
   },
   "outputs": [],
   "source": [
    "## We will create a classifier using embedding layer architecture\n",
    "X=train['TITLE']\n",
    "y=train['CATEGORY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Yg7gcFqu0865"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "tjlSVFK_0866"
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Tquolg-r0867"
   },
   "outputs": [],
   "source": [
    "enc=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6301KDG90867"
   },
   "outputs": [],
   "source": [
    "y_train=enc.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-YVWu1720868",
    "outputId": "2fd2a210-1dbe-4760-c8a1-c8390b30448b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['b', 'e', 'm', 't'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LhAOL6-7086-",
    "outputId": "c1f72d75-6dac-4b48-ea3a-1433d06ef1b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 3, ..., 3, 1, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "AivBLATi086_"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "KIacNtHj087A"
   },
   "outputs": [],
   "source": [
    "seq_len=16\n",
    "max_words=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q9Vho8OI087A",
    "outputId": "83063b4b-7aea-49de-8f8b-4766c96f2d42"
   },
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(num_words=max_words)\n",
    "### Split the text into words and assign an integer id\n",
    "tokenizer.fit_on_texts(X_train.tolist())\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "AY1LzQSH087B"
   },
   "outputs": [],
   "source": [
    "## Create a sequence for each entry in the title column\n",
    "sequence=tokenizer.texts_to_sequences(X_train.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AKEY4eMd087C",
    "outputId": "26613403-6d29-4579-a1cf-4aae7f89cc2d"
   },
   "outputs": [],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "IbqJJLuU087C"
   },
   "outputs": [],
   "source": [
    "## Pad the sequences\n",
    "train_features=pad_sequences(sequence,maxlen=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FEginekd087D",
    "outputId": "29f2ce6e-f7e0-47ed-9622-0218379e479a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,  142, 1562, 8052],\n",
       "       [   0,    0,    0, ...,    4, 1671,  525],\n",
       "       [   0,    0,    0, ..., 5370,    6,   47],\n",
       "       ...,\n",
       "       [   0,    0,    0, ..., 4732, 1042,  359],\n",
       "       [   0,    0,    0, ...,   46,   41,   80],\n",
       "       [   0,    0,    0, ..., 2953, 6426, 2189]], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Yji9Cnd087E",
    "outputId": "f376eb10-2c5d-4ef3-bcac-90b4989f947b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168967, 16)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "sLsCpWaS087E"
   },
   "outputs": [],
   "source": [
    "## Create test features\n",
    "sequence=tokenizer.texts_to_sequences(X_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t4qDxKRq087F",
    "outputId": "b8444f09-47d8-4d78-dacc-07d30ee99f31"
   },
   "outputs": [],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Y4z1IWAn087G"
   },
   "outputs": [],
   "source": [
    "test_features=pad_sequences(sequence,maxlen=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WQIIBWq7087G",
    "outputId": "1d257187-9e6a-482c-aa14-cf3b3c14c03b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,  113,    2,   31],\n",
       "       [   0,    0,    0, ...,    4, 4018, 3115],\n",
       "       [   0,    0,    0, ...,  375, 5948, 4400],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,   11,  157, 1648],\n",
       "       [   0,    0,    0, ...,   97,   76,    7],\n",
       "       [   0,    0,    0, ...,  310, 3979, 5986]], dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1vTnLJog087G",
    "outputId": "440971db-7b1c-43e2-80a4-33fd36b32fcf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42242, 16)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "w4l5QJCd087H"
   },
   "outputs": [],
   "source": [
    "## Convert y_test and y_train to one hot encoded vector\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "WIEctq1V087H"
   },
   "outputs": [],
   "source": [
    "y_train=to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JvJE_wXT087H",
    "outputId": "4e171b69-c852-42ac-aeaa-7e7ae47692a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "e9ExRCBV087I"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NGz6a_u91M7"
   },
   "source": [
    "Now what I will do is I will instantiate the weights of the embedding matrix using pre- train word embedding’s. These word embedding’s can be downloaded from this particular link:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2WEugEOu087I"
   },
   "source": [
    "Can be downloaded from http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Khp5WD1n-DI4"
   },
   "source": [
    "If you use this link you will be able to download a couple of text files. Each text file contains word embedding’s of different dimensionality. For the purposes of this code demo, we will be working with word vectors that have a dimension of 50— that each word vector has 50 elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39l8usCg-VIi"
   },
   "source": [
    "Now, before we read in word vectors into our memory and do something over It, what we will do is: I will first demonstrate how the word vectors look like. \n",
    "\n",
    "(open sample_word_vecs.txt)\n",
    "\n",
    "When you download pre-train word vectors they are usually present in a text file and the usual format followed is this that you will have a word which will be space separated by word vectors. For example, in this particular example “the” is represented by these numbers; “is” is represented by these numbers. Now, when you use pre trained word vectors the dimensionality would either be 50, 60, 100,150, or 300. This text file should only be seen as an example as to how the word vectors are stored once you download them onto your system.\n",
    "\n",
    "Here, I will read the word vectors into memory. What I will do is: will populate a dictionary with keys being the words and the values of the word vectors being the values in the dictionary. So, let me run this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mupDZ2eI087L",
    "outputId": "fcf8dcd0-5178-4ef7-cdbd-3ca2138ccaf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 7.981776475906372 seconds to load glove word vectors\n"
     ]
    }
   ],
   "source": [
    "### Read glove word vectors\n",
    "t0=time.time()\n",
    "embedding_index={}\n",
    "con=open(os.path.join(BASE_DIR,'glove.6B.50d.txt'),encoding='utf-8')\n",
    "for line in con: ##looping over each line\n",
    "    values=line.split() ##splitting it by space\n",
    "    word=values[0] ##first value in each line is the word itself\n",
    "    vector=np.asarray(values[1:],dtype='float32') ##everything else is the word vector\n",
    "    embedding_index[word]=vector ##populate the dict\n",
    "con.close()\n",
    "t1=time.time()\n",
    "print(\"Took {} seconds to load glove word vectors\".format(t1-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTeDdved-_uN"
   },
   "source": [
    "Within this code, I am reading the pre-trained vectors file as a text file and I am looping over each line and splitting it by space. The first value in each line is the word itself and everything else are the word vectors which I am converting into NumPy array for each line and here I'm populating my dictionary.\n",
    "\n",
    "Now let’s take a look at all the keys in the word vectors at we have just read in. Now these are all the key in my dictionary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HOnYXRs6087M",
    "outputId": "95cd7f54-a87f-498a-977a-273ed6ccc88d"
   },
   "outputs": [],
   "source": [
    "embedding_index.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ToDKRXaX_KdO"
   },
   "source": [
    "Now, you can see there is a word called Japan. Let's figure out what is the vector associated with Japan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M2mN98J_087N",
    "outputId": "b7169285-dc11-48ca-cfea-73a3adbd9f56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.31739 , -0.14033 ,  0.32292 ,  1.072   ,  0.33008 ,  0.39406 ,\n",
       "       -0.016682,  0.076903, -0.74591 , -0.31521 ,  1.0033  , -0.12659 ,\n",
       "        0.063252,  0.64006 ,  0.70721 ,  0.84303 , -0.68832 ,  0.47214 ,\n",
       "       -0.66002 ,  0.73962 ,  1.1116  , -0.89428 , -0.90364 , -0.47281 ,\n",
       "        0.88529 , -2.0194  ,  0.30623 , -0.31662 , -0.44423 , -0.52139 ,\n",
       "        3.0287  ,  0.70315 ,  0.92315 ,  0.52263 , -0.62674 , -0.58995 ,\n",
       "       -0.15876 , -0.078332, -1.0794  , -0.71552 , -1.2764  , -0.85554 ,\n",
       "        1.2827  , -1.2134  ,  1.0125  ,  0.40329 , -0.16276 ,  0.99117 ,\n",
       "        0.031016, -0.35431 ], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_index['japan']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJ4E7uka_zE-"
   },
   "source": [
    "These numbers are the word vectors for Japan. \n",
    "\n",
    "Let’s look at its shape as you can see its dimension 50, each vector contains 50 entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qr_wIxY5087N",
    "outputId": "1f5ab68d-25f5-4cfe-fa14-3016b9458d21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_index['japan'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hD6pNeVAACXg"
   },
   "source": [
    "Now the next step that we will do is we will create an embedding weight matrix which will contain weights and those weights will be the values of the word vectors corresponding to the words that occur in my corpus. So first time I am creating an empty matrix containing only zeros with 10,000 rows and 50 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "LP0SetGo087O"
   },
   "outputs": [],
   "source": [
    "## Now create an embedding matrix for 10000 words in our corpus\n",
    "embedding_weight_matrix=np.zeros((max_words,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oR_bEkf_087O",
    "outputId": "61651242-4931-4ddb-e2e4-1648d212e2a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 50)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_weight_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84YDf65LAIGn"
   },
   "source": [
    "Now here, I will loop over the words that are there in my corpus and I will update this embedding weight matrix with the word vectors that I obtained from pre-trained word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "orAfgNKO087O"
   },
   "outputs": [],
   "source": [
    "for word,i in tokenizer.word_index.items():\n",
    "    if i < max_words:\n",
    "        vector=embedding_index.get(word)\n",
    "        if vector is not None:\n",
    "            embedding_weight_matrix[i]=vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JXiXy6zANbm"
   },
   "source": [
    " This is how my embedding weight matrix looks like and this is its shape. It will have 10,000 rows and 50 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WVkAYZ4K087P",
    "outputId": "308e74d1-2190-4c09-e814-aef7a9ab9767"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.68046999, -0.039263  ,  0.30186   , ..., -0.073297  ,\n",
       "        -0.064699  , -0.26043999],\n",
       "       [ 0.33041999,  0.24995001, -0.60873997, ..., -0.50703001,\n",
       "        -0.027273  , -0.53285003],\n",
       "       ...,\n",
       "       [-0.72750998,  0.85914999, -2.07520008, ..., -0.24068999,\n",
       "        -0.67565   , -1.02989995],\n",
       "       [ 0.32872999,  0.19727001,  1.80250001, ...,  0.86822999,\n",
       "         0.30015001,  0.45583001],\n",
       "       [-0.95811999,  0.56607002,  0.24886   , ..., -0.43867001,\n",
       "        -0.50740999,  1.02049994]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ujwQSJT087P",
    "outputId": "9dd3af26-ae23-4225-dfaa-8c4c340ae1de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 50)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_weight_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "9idbDv84087Q"
   },
   "outputs": [],
   "source": [
    "## Now we will assemble the model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "aDWwtfvB087Q"
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(input_dim=max_words,output_dim=50,\n",
    "                    weights=[embedding_weight_matrix],\n",
    "                    input_length=seq_len))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024,activation='relu'))\n",
    "model.add(Dense(4,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNquQYOQApkO"
   },
   "source": [
    "I will need sequential which is the entry point to Keras API. I will be including dense layer. I'll be including an embedding layer whose weights will be the embedding weight matrix that I have just created and I will also need to flatten the embedding layer, so that I can connect it to a dense layer. So here I'm assembling my model. Now you can see the input dimension here is equal to 10,000- if you remember my maximum vocabulary is 10,000. The output dimension is 50 the reason for that is each of the word vectors that I have as only a length of 50. The input length is going to be the length of the sequence if you remember I have completed my sequences to a length of 16 and here I'm instantiating weights. So I am saying the weights of this embedding layer are going to be this object. Now if you remember this object was something that I generated here. Then I flatten my embedding layer and connected to dense layer in the end I have again a dense layer with 4 neurons and a Softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LQu2ZCAR087R",
    "outputId": "01b272d9-b4e4-4cdb-d7c7-96896dd3cf3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 16, 50)            500000    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              820224    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 4100      \n",
      "=================================================================\n",
      "Total params: 1,324,324\n",
      "Trainable params: 1,324,324\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7Sq7gXMBBo1"
   },
   "source": [
    "Now you can see all the parameters in my model are trainable. But what I need to do is I need to make sure that the parameters in the embedding layer, they become fixed and embedding layer is used in the same sense as the convolutional blocks were used in transfer learning. Now to do that what I will do is: I will access the layers property and since embedding is my first layer which indexed as 0, I will change its trainable property false. Now, if I look at my model summary you can now see the parameters in the embedding layer are non-trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "cHvllB8X087R"
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zcWJCtUp087S",
    "outputId": "ca4e2049-2a9b-49b4-9782-4d6a2ec5bee0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 16, 50)            500000    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              820224    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 4100      \n",
      "=================================================================\n",
      "Total params: 1,324,324\n",
      "Trainable params: 824,324\n",
      "Non-trainable params: 500,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "l0-ZWo1y087S"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iqspyAop087S",
    "outputId": "8aef3513-3ee5-4d2f-a9fd-05e19923529a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4225/4225 [==============================] - 52s 12ms/step - loss: 0.4900 - accuracy: 0.8236 - val_loss: 0.4499 - val_accuracy: 0.8427\n",
      "Epoch 2/3\n",
      "4225/4225 [==============================] - 52s 12ms/step - loss: 0.3551 - accuracy: 0.8806 - val_loss: 0.4683 - val_accuracy: 0.8528\n",
      "Epoch 3/3\n",
      "4225/4225 [==============================] - 53s 12ms/step - loss: 0.2472 - accuracy: 0.9188 - val_loss: 0.5665 - val_accuracy: 0.8495\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7f9f5c0090>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_features,y_train,epochs=3,batch_size=32,validation_split=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQgjHukiBoOr"
   },
   "source": [
    "Now this will take some time I'm running my model only for 3 epochs. Now after this model trains we can see it has achieved the validation accuracy of 84%. Let’s obtain predictions from this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S2cVskGB087T",
    "outputId": "112c2851-daa4-4d9c-b624-fba24f88d1ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.9241997e-01, 1.9895509e-02, 1.3075784e-02, 1.7460869e-01],\n",
       "       [7.5139076e-01, 6.9886526e-05, 1.6313244e-06, 2.4853779e-01],\n",
       "       [3.4345221e-10, 3.4921861e-12, 4.5142750e-14, 1.0000000e+00],\n",
       "       ...,\n",
       "       [9.9993479e-01, 1.8836440e-07, 7.7958804e-09, 6.4978631e-05],\n",
       "       [4.1593373e-02, 1.9883511e-05, 2.3821981e-04, 9.5814860e-01],\n",
       "       [3.1795645e-01, 2.8387917e-02, 1.0119211e-01, 5.5246353e-01]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds=model.predict(test_features)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oh6oRmypCJLl"
   },
   "source": [
    "Fine the max probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "pzZ1C-D1B_cB"
   },
   "outputs": [],
   "source": [
    "max_labels = []\n",
    "for i in preds:\n",
    "  max_labels.append(np.argmax(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3iiPkasACM4s"
   },
   "source": [
    "Inverse transform them into their actual labels and here I'm computing the accuracy on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "7WVc7p3X087T"
   },
   "outputs": [],
   "source": [
    "pred_labels=enc.inverse_transform(np.array(max_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mKVnLgGO087T",
    "outputId": "1d5136ac-a26b-461e-dffe-0896c47b1efa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84768714])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_test==pred_labels).sum()/pred_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdTVfYqqCU2m"
   },
   "source": [
    " So, the accuracy on the test data is around 84%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r3V3bWK1087U"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Pre_trained_embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
