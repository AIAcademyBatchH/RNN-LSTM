{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFXFLwZ8d56D"
   },
   "source": [
    "In this code demo we will see how we can use TFIDF vector representation to do text classification along with a multilayered preceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2f1V0Npyd4-I",
    "outputId": "255d3788-f0cd-4839-aa08-fc4303d94d25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zLcrV5DOdetq"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eyJxPJj0det2"
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"/content/gdrive/MyDrive/RNN-LSTM/stack-overflow-data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "aII56ZoDdet4",
    "outputId": "5c0e5c5e-e598-403f-ce00-41277b2492d5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is causing this behavior  in our c# datet...</td>\n",
       "      <td>c#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>have dynamic html load as if it was in an ifra...</td>\n",
       "      <td>asp.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how to convert a float value in to min:sec  i ...</td>\n",
       "      <td>objective-c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.net framework 4 redistributable  just wonderi...</td>\n",
       "      <td>.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trying to calculate and print the mean and its...</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post         tags\n",
       "0  what is causing this behavior  in our c# datet...           c#\n",
       "1  have dynamic html load as if it was in an ifra...      asp.net\n",
       "2  how to convert a float value in to min:sec  i ...  objective-c\n",
       "3  .net framework 4 redistributable  just wonderi...         .net\n",
       "4  trying to calculate and print the mean and its...       python"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6qASTZKe5JL"
   },
   "source": [
    "Let’s try and understand what this data set is about. This data set has a collection of questions which were asked on stack overflow as well as their corresponding tag. For example, here I know that, this question was about .net; this question was about Python; this question was about C sharp. The task here is to take the question text as an input and try to predict what would be the tag of that question. For this, what I will do is I will separate out the question as well as the tag into two different objects X and Y. Next I will split my data into test and training components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "iVKHoMnTdet6"
   },
   "outputs": [],
   "source": [
    "X=data['post']\n",
    "y=data['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JtxsEzdQdet7"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6fqJ6m-rdet8"
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.10,random_state=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "97XGA7U6det9",
    "outputId": "fa81c0b8-b763-4d1d-8af0-8fa2fcbf7e4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21805    is there an library to support nsdictionary tr...\n",
       "25783    replacing fileinputstream with getresourceasst...\n",
       "34843    how to email the current screen of ios app in ...\n",
       "16332    how to create the nscalender and gregorian cal...\n",
       "32383    is it possible to check what the client is sen...\n",
       "Name: post, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6IukaP0mfYlQ"
   },
   "source": [
    "I'll try to create a TFIDF representation of all the questions in the train set as well as in the test set. For that, I will be using the text module within feature extraction module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Vq2wtomQdet_"
   },
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction.text as text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGrr5hQ1fepj"
   },
   "source": [
    "This text module contains a function called TFIDF vectorizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "dtEXUeV9deuB"
   },
   "outputs": [],
   "source": [
    "tfidf=text.TfidfVectorizer(input=X_train.tolist(),stop_words='english',max_features=1000,ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGoIsMDVfwMC"
   },
   "source": [
    "It accepts a list as an input here I'm giving the list of questions in my train set as an input.\n",
    "\n",
    "I want to filter out all the common English stop words. This is what I'm doing here. Now when you created TFIDF vector or a TFIDF matrix the number of columns in a TFIDF matrix can become very large. Here we are putting an artificial restriction that are TFIDF matrix you only have 1000 columns. And here I’m using only unigrams. So let's create an object of TFIDF vectorizer class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlfpHMIIgEe_"
   },
   "source": [
    "Now I will pass again my questions as an input in the form of a list to this object. This object has a fit transform () method. Now this will create a TFIDF representation for all the questions in my train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "N4G3EFMQdeuL"
   },
   "outputs": [],
   "source": [
    "X_train=tfidf.fit_transform(X_train.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59swTS3ugHfk"
   },
   "source": [
    "Similarly, I will create TFIDF representation for all the questions in my test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "yY7LLXNSdeuN"
   },
   "outputs": [],
   "source": [
    "X_test=tfidf.transform(X_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5aB-T0_pdeuN",
    "outputId": "80387e9e-1b03-49c4-da64-31a55ff2e8c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36000, 1000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JsP_JsfXdeuP",
    "outputId": "fcd27493-d773-4cd9-ec92-1d02e4dc8fdf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 1000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RQ4LQbrVdeuP",
    "outputId": "b535084b-f04a-4936-cde5-e2502d80905e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             c#\n",
       "1        asp.net\n",
       "2    objective-c\n",
       "3           .net\n",
       "4         python\n",
       "Name: tags, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tO1ba4AgZu0"
   },
   "source": [
    "Now let's take a look at our y vector. You can see our y vector contains the tags. Now in order for us to use neural networks we will need to convert these tags into some integer representation and then do a one hot encoding on them. To this effect, we will use the label encoder class and create an object of this class and fit all the tags on this class and obtain a transformed version of the tags in my train set as well as my test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "0rhZIu-NdeuQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "QzRMNd2bdeuR"
   },
   "outputs": [],
   "source": [
    "enc=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LF0cSt-ldeuR",
    "outputId": "b9b73389-5c0f-4427-e978-4873780c0556"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.fit(data['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "JzQKq625deuX"
   },
   "outputs": [],
   "source": [
    "y_train=enc.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "8dywyxesdeuX"
   },
   "outputs": [],
   "source": [
    "y_test=enc.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fEG8j6lHdeuY",
    "outputId": "88707ee6-8a23-410e-a3b0-aae4690e6107"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15, 11, 15, ...,  5,  3, 18])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ak0sUCxMgtR8"
   },
   "source": [
    "Now you can see all the tags have been converted into a number and the mapping can be understood using encoded.classes attribute. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAy5Xrb2gwZa"
   },
   "source": [
    "So this dot classes attribute tells what integer coding has been given to which class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0S7P6RZJdeuZ",
    "outputId": "69e2b24f-4e72-43cf-b915-b5afe7bf8ef4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['.net', 'android', 'angularjs', 'asp.net', 'c', 'c#', 'c++', 'css',\n",
       "       'html', 'ios', 'iphone', 'java', 'javascript', 'jquery', 'mysql',\n",
       "       'objective-c', 'php', 'python', 'ruby-on-rails', 'sql'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90F8Wa3shTnM"
   },
   "source": [
    "Now, we will create a simple multi-layered perceptron architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "YVtDtSeNdeua"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auTnLleth0kx"
   },
   "source": [
    "Now I will have to create a one hot encoded version of the y vector in my training data for which I will use to_categorical ()function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "wG2h0lptdeub"
   },
   "outputs": [],
   "source": [
    "y_train=to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YQAEjd-Xdeuc",
    "outputId": "957c0952-706c-43a8-cac2-eb3e6319bbd1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "TbdXDzCCdeuc"
   },
   "outputs": [],
   "source": [
    "model=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "_j7twHahdeud"
   },
   "outputs": [],
   "source": [
    "model.add(Dense(512,input_shape=(1000,),activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(20,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wo3gt6ykiqK6"
   },
   "source": [
    "My model will have two dense layers and a drop out layer. I will compile my model now and fit on my data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "FoP-jmO8deud"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gfBqlM4adeue",
    "outputId": "1cac6abe-66e2-4ef7-ef89-42dbb273ed2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1013/1013 [==============================] - 6s 5ms/step - loss: 1.1617 - accuracy: 0.7050 - val_loss: 0.6728 - val_accuracy: 0.8011\n",
      "Epoch 2/2\n",
      "1013/1013 [==============================] - 4s 4ms/step - loss: 0.6299 - accuracy: 0.8015 - val_loss: 0.6394 - val_accuracy: 0.8036\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1bf76bc150>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train.toarray(),y_train,epochs=2,batch_size=32,verbose=1,validation_split=0.10)\n",
    "## x_train is a sparse matrix - convert toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97LUU0p8iyB3"
   },
   "source": [
    "Now after I fit my model for only two epochs, I can see after two epochs the out of sample accuracy is around 80%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRQxctI9jbxk"
   },
   "source": [
    "Now to test my model what I will do is: I have taken a stack overflow question. Let's create a string out of this question. This string is stored in object Q. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "wcBRnxsTdeue"
   },
   "outputs": [],
   "source": [
    "q='''\n",
    "\n",
    "I am new to luigi, came across it while designing a pipeline for our ML efforts and though it wasn't fitted to my particular use case it had so many extra features I decided to fit it to my use case, basically what I was looking for was a way to be able to persist a custom built pipeline and thus have its result repeatable, after reading most of the online tutorials I tried to implement my serialization using the existing luigi.cfg configuration and command line mechanisms and it might have sufficed for the tasks' parameters but it provided no way of serializing the DAG connectivity of my pipeline, so I decided to have a WrapperTask which received a json config file which would create all the task instances and connect all the plumbing. I hereby enclose a small test program for your scrutiny:\n",
    "\n",
    "import random\n",
    "import luigi\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "class TaskNode(luigi.Task):\n",
    "    i = luigi.IntParameter()  # node ID\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.required = []\n",
    "\n",
    "    def set_required(self, required=None):\n",
    "        self.required = required  # set the dependencies\n",
    "        return self\n",
    "\n",
    "    def requires(self):\n",
    "        return self.required\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget('{0}{1}.txt'.format(self.__class__.__name__, self.i))\n",
    "\n",
    "    def run(self):\n",
    "        with self.output().open('w') as outfile:\n",
    "            outfile.write('inside {0}{1}\\n'.format(self.__class__.__name__, self.i))\n",
    "        self.process()\n",
    "\n",
    "    def process(self):\n",
    "        raise NotImplementedError(self.__class__.__name__ + \" must implement this method\")\n",
    "\n",
    "\n",
    "class FastNode(TaskNode):\n",
    "\n",
    "    def process(self):\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "class SlowNode(TaskNode):\n",
    "\n",
    "    def process(self):\n",
    "        time.sleep(2)\n",
    "\n",
    "\n",
    "# This WrapperTask builds all the nodes \n",
    "class All(luigi.WrapperTask):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        num_nodes = 513\n",
    "\n",
    "        classes = TaskNode.__subclasses__()\n",
    "        self.nodes = []\n",
    "        for i in reversed(range(num_nodes)):\n",
    "            cls = random.choice(classes)\n",
    "\n",
    "            dependencies = random.sample(self.nodes, (num_nodes - i) // 35)\n",
    "\n",
    "            obj = cls(i=i)\n",
    "            if dependencies:\n",
    "                obj.set_required(required=dependencies)\n",
    "            else:\n",
    "                obj.set_required(required=None)\n",
    "\n",
    "            # delete existing output causing a build all\n",
    "            if obj.output().exists():\n",
    "                obj.output().remove()  \n",
    "\n",
    "            self.nodes.append(obj)\n",
    "\n",
    "    def requires(self):\n",
    "        return self.nodes\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    luigi.run()\n",
    "So, basically, as states in the question title, this focuses on the dynamic dependencies and generates a 513 node dependency DAG with p=1/35 connectivity probability, it also makes the All WrapperTask class require all nodes (I have a version which only connects it to heads of connected DAG components but I didn't want to over complicate).\n",
    "\n",
    "Is there a more standard way of implementing this? Especially note the not so pretty complication with the TaskNode init and set_required methods, I only did it this way because receiving parameters in the init method clashes somehow with the way luigi registers parameters. I also tried several other ways but this was basically the most decent one (that worked)\n",
    "\n",
    "If there isn't a standard way I'd still love to hear any insights you have on the way I plan to go before I implement the entire piping framework.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DqBXWjOgjjwO"
   },
   "source": [
    "Now what I will do is I will create a TFIDF representation of this text and I will pass this TFIDF representation as an input to get a prediction from my model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "fc5BcH2cdeuq"
   },
   "outputs": [],
   "source": [
    "q_v=tfidf.transform([q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L37p7Sb6lxrS",
    "outputId": "ce460914-e2d0-4694-9f9f-2e11494a08a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 78 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JaUb_GKydeur",
    "outputId": "4a0f3d9b-83f7-446c-aafa-1b59a3aa7d00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.0773132e-05, 9.6104111e-07, 2.2326586e-07, 6.2382577e-07,\n",
       "        2.8022289e-06, 2.0339468e-05, 3.3024339e-06, 1.3054367e-07,\n",
       "        3.6693990e-07, 2.9894814e-04, 1.3144929e-05, 1.6850501e-05,\n",
       "        2.1006726e-06, 5.6127288e-08, 2.9671153e-07, 2.3677091e-04,\n",
       "        3.3554090e-06, 9.9933904e-01, 3.7818296e-05, 2.1792146e-06]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(q_v.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c-uZd94el7HA",
    "outputId": "a0f5398e-9718-40cd-8124-e5a46c58b96f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model.predict(q_v.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0RPW3zzjn9T"
   },
   "source": [
    "Now my model is predicting this to be of class 17, let's try and understand what class 17 stands for we can use the inverse_transform() method within our encoded class to find out the class of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dPfLHq05deur",
    "outputId": "8d7908bc-d819-49e5-ef55-358b103245f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['python'], dtype=object)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.inverse_transform([17])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNbccagtj7fW"
   },
   "source": [
    "So my model is predicting that this question is about python and if you look at the code and look at the imports, this is actually a Python question and my model has been able to successfully identify a python question from its text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8NR9o04Jdeus"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tfidf_mlp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
