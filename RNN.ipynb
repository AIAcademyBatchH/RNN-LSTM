{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TP3aSP2cK-QT"
   },
   "source": [
    "In this code demo, we will see how we can build recurrent neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-0Kp9PSNCw_b",
    "outputId": "c693c1b0-cadf-4424-b4da-6c5b812aa9de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VEni0jEACwJo"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "BASE_DIR=\"/content/gdrive/MyDrive/RNN-LSTM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dwxPCzZ3CwJv"
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv(os.path.join(BASE_DIR,'headlines.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "jrWHEVkqCwJw",
    "outputId": "0c3d955b-aae9-4eb6-88ae-cdae0415db2c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>CATEGORY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>226435</td>\n",
       "      <td>Google+ rolls out 'Stories' for tricked out ph...</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>356684</td>\n",
       "      <td>Dov Charney's Redeeming Quality</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>246926</td>\n",
       "      <td>White God adds Un Certain Regard to the Palm Dog</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>318360</td>\n",
       "      <td>Google shows off Androids for wearables, cars,...</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>277235</td>\n",
       "      <td>China May new bank loans at 870.8 bln yuan</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID                                              TITLE CATEGORY\n",
       "0  226435  Google+ rolls out 'Stories' for tricked out ph...        t\n",
       "1  356684                    Dov Charney's Redeeming Quality        b\n",
       "2  246926   White God adds Un Certain Regard to the Palm Dog        e\n",
       "3  318360  Google shows off Androids for wearables, cars,...        t\n",
       "4  277235         China May new bank loans at 870.8 bln yuan        b"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ofabgzHSCwJy"
   },
   "outputs": [],
   "source": [
    "## We will create a classifier using embedding layer and Recurrent layer\n",
    "X=train['TITLE']\n",
    "y=train['CATEGORY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "F0JqX1qhCwJz"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "m51GVRw6CwJz"
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "cDrPCVkICwJ0"
   },
   "outputs": [],
   "source": [
    "enc=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZF98rlN7CwJ1"
   },
   "outputs": [],
   "source": [
    "y_train=enc.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tJoXqvpDCwJ2",
    "outputId": "889c4919-80d3-4bb5-e668-bc5c7348d42f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['b', 'e', 'm', 't'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qTrg2VvOCwJ3",
    "outputId": "2a7d8bfa-3b14-4c3f-eca0-b0744e3fce0b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 3, ..., 3, 1, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "WRv_prVgCwJ4"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "vbNoylKbCwJ5"
   },
   "outputs": [],
   "source": [
    "seq_len=16\n",
    "max_words=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ONHBs4ObCwJ6"
   },
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(num_words=max_words)\n",
    "### Split the text into words and assign an integer id\n",
    "tokenizer.fit_on_texts(X_train.tolist())\n",
    "## Create a sequence for each entry in the title column\n",
    "sequence=tokenizer.texts_to_sequences(X_train.tolist())\n",
    "## Pad the sequences\n",
    "train_features=pad_sequences(sequence,maxlen=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OoGUcIRCCwJ7",
    "outputId": "f1f070a3-f86c-42a7-b425-9a52ed85efa6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,  142, 1562, 8052],\n",
       "       [   0,    0,    0, ...,    4, 1671,  525],\n",
       "       [   0,    0,    0, ..., 5370,    6,   47],\n",
       "       ...,\n",
       "       [   0,    0,    0, ..., 4732, 1042,  359],\n",
       "       [   0,    0,    0, ...,   46,   41,   80],\n",
       "       [   0,    0,    0, ..., 2953, 6426, 2189]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pY9iuUjZCwJ8",
    "outputId": "1b51db24-ce07-43ee-8c38-e5cc16fc126a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168967, 16)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XFYqtAhpCwJ8",
    "outputId": "0031086d-8bee-44f1-a4fc-e845fc03a992"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,  113,    2,   31],\n",
       "       [   0,    0,    0, ...,    4, 4018, 3115],\n",
       "       [   0,    0,    0, ...,  375, 5948, 4400],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,   11,  157, 1648],\n",
       "       [   0,    0,    0, ...,   97,   76,    7],\n",
       "       [   0,    0,    0, ...,  310, 3979, 5986]], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create test features\n",
    "sequence=tokenizer.texts_to_sequences(X_test.tolist())\n",
    "test_features=pad_sequences(sequence,maxlen=seq_len)\n",
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0sc5cHWQCwJ9",
    "outputId": "f6c60859-e129-4d18-f5b0-e5ecea8575a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42242, 16)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "i9QNblWBCwJ9"
   },
   "outputs": [],
   "source": [
    "## Convert y_test and y_train to one hot encoded vector\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "6shmThUfCwJ-"
   },
   "outputs": [],
   "source": [
    "y_train=to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WDIECrihCwJ-",
    "outputId": "fb21d652-4555-49d2-cd4e-7893fed88776"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "dZDI-_W9CwJ_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fU80RuZ9CwJ_",
    "outputId": "b78a4ab2-18aa-4da6-e50f-7f87ed8fcfb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 9.959806442260742 seconds to load glove word vectors\n"
     ]
    }
   ],
   "source": [
    "### Read glove word vectors\n",
    "t0=time.time()\n",
    "embedding_index={}\n",
    "con=open(os.path.join(BASE_DIR,'glove.6B.50d.txt'),encoding='utf-8')\n",
    "for line in con:\n",
    "    values=line.split()\n",
    "    word=values[0]\n",
    "    vector=np.asarray(values[1:],dtype='float32')\n",
    "    embedding_index[word]=vector\n",
    "con.close()\n",
    "t1=time.time()\n",
    "print(\"Took {} seconds to load glove word vectors\".format(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "tlSuAs2sCwKA"
   },
   "outputs": [],
   "source": [
    "## Now create an embedding matrix for 10000 words in our corpus\n",
    "embedding_weight_matrix=np.zeros((max_words,50))\n",
    "for word,i in tokenizer.word_index.items():\n",
    "    if i < max_words:\n",
    "        vector=embedding_index.get(word)\n",
    "        if vector is not None:\n",
    "            embedding_weight_matrix[i]=vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbBhaxavL-x1"
   },
   "source": [
    "Now, I will start assembling my model. In this model, besides, the dense layers and the embedding layer, I will also use a simple recurrent layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "tt2HFAzXCwKB"
   },
   "outputs": [],
   "source": [
    "## Now we will assemble the model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding,SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "YuQdtLQpCwKB"
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(input_dim=max_words,output_dim=50,\n",
    "                    weights=[embedding_weight_matrix],\n",
    "                    input_length=seq_len))\n",
    "model.add(SimpleRNN(50))\n",
    "model.add(Dense(4,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdt3LMTUMJch"
   },
   "source": [
    "Now the entry point to the model is sequential. The first layer in the embedding layer. The number of rows in this embedding layer is same as the maximum vocabulary. The output from this embedding layer is 50 because I'm using word vectors that have a length of 50. I will have to instantiate the weights that I created here. and I will also have to specify the length of input which is my sequence length which is 16 in this case. Then I include a simple recurrent layer. If you remember the recurrent layer takes in the output of the embedding layer. And embedding layer is going to produce word vectors as the output whose dimension will be 50. That's why you have a number 50 here. The last layer will be dense layer. We have four categories, so we will be using four neurons with soft Max activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PsC_ENckCwKB",
    "outputId": "7c489ed4-b019-4408-ce1f-b2cf4ce49b30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 16, 50)            500000    \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 204       \n",
      "=================================================================\n",
      "Total params: 505,254\n",
      "Trainable params: 505,254\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oR9iuv3yMn3M"
   },
   "source": [
    "Now you can see that all the parameters in the model are trainable. We may want to freeze these parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Cdc6vTldCwKC"
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Skucr5lCwKC",
    "outputId": "c93a0da5-2f71-4996-d4da-8c7cf3ad7f17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 16, 50)            500000    \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 204       \n",
      "=================================================================\n",
      "Total params: 505,254\n",
      "Trainable params: 5,254\n",
      "Non-trainable params: 500,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jn6DHYCdMqdy"
   },
   "source": [
    "if you again look at the summary I can see there are five lakh parameters which are non-trainable which belong to my embeded layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "H4him11KCwKC"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B1iGE37-CwKD",
    "outputId": "1f1340a1-aa44-42c3-a715-7266f0516131"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4225/4225 [==============================] - 21s 5ms/step - loss: 0.5561 - acc: 0.7963 - val_loss: 0.4939 - val_acc: 0.8209\n",
      "Epoch 2/3\n",
      "4225/4225 [==============================] - 20s 5ms/step - loss: 0.4738 - acc: 0.8287 - val_loss: 0.4680 - val_acc: 0.8308\n",
      "Epoch 3/3\n",
      "4225/4225 [==============================] - 19s 5ms/step - loss: 0.4369 - acc: 0.8424 - val_loss: 0.4910 - val_acc: 0.8174\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faa6b9c7850>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_features,y_train,epochs=3,batch_size=32,validation_split=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20hnfeNnM0ew"
   },
   "source": [
    "Val Accuracy is around 81%\n",
    "\n",
    "Let's look at test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mRdgxLNUCwKD",
    "outputId": "d77ce14a-878a-4b84-e1f4-955562e8370c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8159414800435586"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds=model.predict(test_features)\n",
    "\n",
    "max_labels = []\n",
    "for i in preds:\n",
    "  max_labels.append(np.argmax(i))\n",
    "\n",
    "pred_labels=enc.inverse_transform(np.array(max_labels))\n",
    "(y_test==pred_labels).sum()/pred_labels.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6r6HXyYCM7i3"
   },
   "source": [
    "The accuracy on the test data is again 81%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cc1qbowiL7-p"
   },
   "source": [
    "Now you can have a recurrent neural network with more than one simple recurrent layer. For that to happen we have to make sure that the recurrent layers preceding to the final recurrent layer have a parameter called return_sequences() to which a value of true is provided. Let's assemble a recurrent neural network with three recurrent layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "RAWL1wGiCwKD"
   },
   "outputs": [],
   "source": [
    "## One can have an rnn model with more than one recurrent layer, when this is done the intermediate layers should return\n",
    "# the sequence of states not just last state.\n",
    "model=Sequential()\n",
    "model.add(Embedding(input_dim=max_words,output_dim=50,\n",
    "                    weights=[embedding_weight_matrix],\n",
    "                    input_length=seq_len))\n",
    "model.add(SimpleRNN(50,return_sequences=True))\n",
    "model.add(SimpleRNN(50,return_sequences=True))\n",
    "model.add(SimpleRNN(50))\n",
    "model.add(Dense(4,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_a_tT-lNGRm"
   },
   "source": [
    "return_sequences:  Whether to return the last output in the output sequence, or the full sequence.\n",
    "\n",
    "Let's make the embedding layer non-trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "fvoDSjlFCwKE"
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtSmowxVNImo"
   },
   "source": [
    "Letâ€™s look at the summary and let's compile a model and run it for three epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BpELTXfzCwKE",
    "outputId": "167daafe-e3df-442a-d199-fef2d0673391"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 16, 50)            500000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 16, 50)            5050      \n",
      "_________________________________________________________________\n",
      "simple_rnn_2 (SimpleRNN)     (None, 16, 50)            5050      \n",
      "_________________________________________________________________\n",
      "simple_rnn_3 (SimpleRNN)     (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 204       \n",
      "=================================================================\n",
      "Total params: 515,354\n",
      "Trainable params: 15,354\n",
      "Non-trainable params: 500,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "JzMQNzgDCwKE"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sdi8BQXUCwKF",
    "outputId": "7f7da001-6182-483b-c9cd-e01f0cad2fc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4225/4225 [==============================] - 56s 13ms/step - loss: 0.5367 - acc: 0.8017 - val_loss: 0.5460 - val_acc: 0.8024\n",
      "Epoch 2/3\n",
      "4225/4225 [==============================] - 53s 13ms/step - loss: 0.4507 - acc: 0.8360 - val_loss: 0.4615 - val_acc: 0.8309\n",
      "Epoch 3/3\n",
      "4225/4225 [==============================] - 53s 13ms/step - loss: 0.4144 - acc: 0.8507 - val_loss: 0.4921 - val_acc: 0.8274\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faa6b772c50>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_features,y_train,epochs=3,batch_size=32,validation_split=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPm_57IBNN4S"
   },
   "source": [
    "Now once we are done training with this network, you can see that the validation accuracy is now around 82%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RyqpdBrVCwKF",
    "outputId": "1da079ae-b147-4514-e8d4-dc76837f1200"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8308318734908385"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds=model.predict(test_features)\n",
    "\n",
    "max_labels = []\n",
    "for i in preds:\n",
    "  max_labels.append(np.argmax(i))\n",
    "\n",
    "pred_labels=enc.inverse_transform(np.array(max_labels))\n",
    "(y_test==pred_labels).sum()/pred_labels.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qYLlcctNUqj"
   },
   "source": [
    "Our accuracy on the test data is also around 83%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qUDsHn0vCwKF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
